import streamlit as st
import google.generativeai as genai
# --- C·∫¨P NH·∫¨T IMPORT CHU·∫®N (FIX L·ªñI) ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
# ƒê∆∞·ªùng d·∫´n ch√≠nh x√°c nh·∫•t c·ªßa load_qa_chain:
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
# ---------------------------------------
from PyPDF2 import PdfReader
from docx import Document
from PIL import Image
import io
import requests
from bs4 import BeautifulSoup
import time
import zipfile
import os

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Ultimate AI: God Mode (Stable)", page_icon="‚òØÔ∏è", layout="wide")
st.markdown("""<style>.stButton>button {background-color: #8e44ad; color: white;}</style>""", unsafe_allow_html=True)

# --- K·∫æT N·ªêI API & T·ª∞ ƒê·ªòNG QU√âT MODEL ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
    os.environ["GOOGLE_API_KEY"] = api_key
    
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name:
                available_models.append(m.name)
    except: pass
    
    if not available_models:
        available_models = ["models/gemini-1.5-pro", "models/gemini-1.5-flash", "models/gemini-pro"]
        
except:
    st.error("‚ö†Ô∏è Ch∆∞a nh·∫≠p API Key trong Secrets.")
    st.stop()

# --- C√ÅC H√ÄM X·ª¨ L√ù ---

def get_text_from_files(files):
    text = ""
    for f in files:
        if f.name.endswith('.pdf'):
            reader = PdfReader(f)
            for page in reader.pages: text += page.extract_text() or ""
        elif f.name.endswith('.docx'):
            doc = Document(f)
            for para in doc.paragraphs: text += para.text + "\n"
        elif f.name.endswith('.txt'):
            text += f.getvalue().decode("utf-8")
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    chunks = text_splitter.split_text(text)
    return chunks

def create_vector_store(text_chunks):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index_huyenhoc")
    return vector_store

def zip_folder(folder_path, output_path):
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..')))

def scrape_chapter(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        content = "\n".join([p.get_text() for p in soup.find_all('p')])
        if len(content) < 100: content = soup.get_text()
        return content
    except: return ""

def translate_docx_preserve_layout(file, instruction, glossary, model_name):
    doc = Document(file)
    model_trans = genai.GenerativeModel(model_name)
    total_paragraphs = len(doc.paragraphs)
    bar = st.progress(0)
    status = st.empty()
    batch_size = 10
    current_batch = []
    current_indices = []
    
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            current_batch.append(text)
            current_indices.append(i)
        
        if len(current_batch) >= batch_size or (i == total_paragraphs - 1 and current_batch):
            status.text(f"ƒêang d·ªãch ƒëo·∫°n {i}/{total_paragraphs}...")
            batch_text = "\n[--BREAK--]\n".join(current_batch)
            prompt = f"VAI TR√í: Bi√™n d·ªãch vi√™n.\nNHI·ªÜM V·ª§: D·ªãch sang Ti·∫øng Vi·ªát.\nY√äU C·∫¶U: {instruction}\nTHU·∫¨T NG·ªÆ: {glossary}\nL∆ØU √ù: Gi·ªØ nguy√™n s·ªë l∆∞·ª£ng ƒëo·∫°n, ph√¢n c√°ch b·ªüi [--BREAK--].\n\nVƒÇN B·∫¢N G·ªêC:\n{batch_text}"
            try:
                response = model_trans.generate_content(prompt)
                translated_batch = response.text.split("[--BREAK--]")
                for idx, trans_text in zip(current_indices, translated_batch):
                    if idx < len(doc.paragraphs):
                        doc.paragraphs[idx].text = trans_text.strip()
            except: pass
            current_batch = []
            current_indices = []
            bar.progress((i+1)/total_paragraphs)
            time.sleep(1)

    status.text("‚úÖ ƒê√£ d·ªãch xong! ·∫¢nh v√† B·∫£ng bi·ªÉu ƒë∆∞·ª£c gi·ªØ nguy√™n.")
    bio = io.BytesIO()
    doc.save(bio)
    return bio

def save_docx_new(content):
    doc = Document()
    for line in content.split('\n'):
        if line.strip(): doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- GIAO DI·ªÜN CH√çNH ---
st.title("‚òØÔ∏è Ultimate AI: God Mode (Fixed)")

# --- SIDEBAR: C·∫§U H√åNH ---
with st.sidebar:
    st.header("‚öôÔ∏è TRUNG T√ÇM ƒêI·ªÄU KHI·ªÇN")
    selected_model_name = st.selectbox("Ch·ªçn B·ªô N√£o (Model):", available_models, index=0)
    st.success(f"ƒêang d√πng: {selected_model_name}")
    st.divider()
    menu = st.radio("CH·ª®C NƒÇNG:", ["1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)", "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)", "3. D·ªãch Thu·∫≠t ƒêa NƒÉng"])

# --- MODULE 1: HU·∫§N LUY·ªÜN ---
if menu == "1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)":
    st.header("üß† Hu·∫•n Luy·ªán AI")
    uploaded_files = st.file_uploader("N·∫°p s√°ch:", accept_multiple_files=True)
    if st.button("Train & T·∫£i B·ªô N√£o"):
        if uploaded_files:
            with st.spinner("ƒêang h·ªçc..."):
                raw = get_text_from_files(uploaded_files)
                create_vector_store(get_text_chunks(raw))
                zip_folder("faiss_index_huyenhoc", "bo_nao.zip")
                with open("bo_nao.zip", "rb") as fp:
                    st.download_button("üì• T·∫£i B·ªô N√£o V·ªÅ", fp, "bo_nao.zip", "application/zip")

# --- MODULE 2: H·ªéI ƒê√ÅP ---
elif menu == "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)":
    st.header(f"üîÆ H·ªèi ƒê√°p RAG (Model: {selected_model_name})")
    brain = st.sidebar.file_uploader("N·∫°p file 'bo_nao.zip':", type="zip")
    vs = None
    if brain:
        with open("temp.zip", "wb") as f: f.write(brain.getbuffer())
        with zipfile.ZipFile("temp.zip", "r") as z: z.extractall(".")
        vs = FAISS.load_local("faiss_index_huyenhoc", GoogleGenerativeAIEmbeddings(model="models/embedding-001"), allow_dangerous_deserialization=True)
        st.sidebar.success("ƒê√£ n·∫°p n√£o!")
    
    if "msgs" not in st.session_state: st.session_state.msgs = []
    for m in st.session_state.msgs: st.chat_message(m["role"]).markdown(m["content"])
    
    if q := st.chat_input("H·ªèi g√¨ ƒëi..."):
        st.session_state.msgs.append({"role": "user", "content": q})
        st.chat_message("user").markdown(q)
        if vs:
            docs = vs.similarity_search(q, k=4)
            chain = load_qa_chain(ChatGoogleGenerativeAI(model=selected_model_name), chain_type="stuff", prompt=PromptTemplate(template="D·ª±a v√†o s√°ch: {context}\nTr·∫£ l·ªùi: {question}", input_variables=["context", "question"]))
            res = chain({"input_documents": docs, "question": q}, return_only_outputs=True)
            st.session_state.msgs.append({"role": "assistant", "content": res["output_text"]})
            st.chat_message("assistant").markdown(res["output_text"])
        else: st.error("Ch∆∞a n·∫°p b·ªô n√£o!")

# --- MODULE 3: D·ªäCH THU·∫¨T ---
elif menu == "3. D·ªãch Thu·∫≠t ƒêa NƒÉng":
    st.header(f"üè≠ D·ªãch Thu·∫≠t (ƒê·ªông c∆°: {selected_model_name})")
    col_a, col_b = st.columns(2)
    with col_a: instruction = st.text_area("Y√™u c·∫ßu:", value="D·ªãch sang ti·∫øng Vi·ªát.", height=100)
    with col_b: glossary = st.text_area("T·ª´ ƒëi·ªÉn:", value="Insight\nROI", height=100)

    tab1, tab2, tab3 = st.tabs(["üìÑ Word (Gi·ªØ ·∫¢nh)", "üåê Link/Text", "üñºÔ∏è D·ªãch ·∫¢nh"])

    with tab1:
        docx_file = st.file_uploader("T·∫£i file Word:", type=['docx'])
        if docx_file and st.button("üöÄ D·ªãch File"):
            processed = translate_docx_preserve_layout(docx_file, instruction, glossary, selected_model_name)
            st.download_button(f"üì• T·∫£i {docx_file.name}", processed.getvalue(), f"VN_{docx_file.name}", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

    with tab2:
        urls = st.text_area("D√°n Link:")
        if st.button("üöÄ D·ªãch Link"):
            links = urls.split('\n')
            full = ""
            bar = st.progress(0)
            model_t = genai.GenerativeModel(selected_model_name)
            for i, link in enumerate(links):
                if link.strip():
                    raw = scrape_chapter(link.strip())
                    if raw:
                        try:
                            res = model_t.generate_content(f"Y√™u c·∫ßu: {instruction}\nN·ªôi dung: {raw[:15000]}")
                            full += f"\n\n--- {link} ---\n{res.text}"
                        except: pass
                    bar.progress((i+1)/len(links))
            st.download_button("T·∫£i v·ªÅ", save_docx_new(full).getvalue(), "Truyen_Web.docx")

    with tab3:
        imgs = st.file_uploader("T·∫£i ·∫£nh:", accept_multiple_files=True, type=['png', 'jpg'])
        if imgs and st.button("üöÄ D·ªãch ·∫¢nh"):
            full_trans = ""
            model_v = genai.GenerativeModel(selected_model_name)
            for img_file in imgs:
                img = Image.open(img_file)
                st.image(img, width=200)
                try:
                    res = model_v.generate_content([f"D·ªãch sang TV. Y√™u c·∫ßu: {instruction}", img])
                    full_trans += f"\n\n--- {img_file.name} ---\n{res.text}"
                except: pass
            st.text_area("K·∫øt qu·∫£:", full_trans)
            st.download_button("T·∫£i v·ªÅ", save_docx_new(full_trans).getvalue(), "Dich_Anh.docx")
