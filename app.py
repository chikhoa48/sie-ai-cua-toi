import streamlit as st
import google.generativeai as genai
# --- C·∫¨P NH·∫¨T IMPORT M·ªöI ƒê·ªÇ S·ª¨A L·ªñI ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_text_splitters import RecursiveCharacterTextSplitter # <-- ƒê√£ s·ª≠a d√≤ng n√†y
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
# ---------------------------------------
from PyPDF2 import PdfReader
from docx import Document
from PIL import Image
import io
import requests
from bs4 import BeautifulSoup
import time
import zipfile
import os

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Ultimate AI: God Mode", page_icon="‚òØÔ∏è", layout="wide")
st.markdown("""<style>.stButton>button {background-color: #8e44ad; color: white;}</style>""", unsafe_allow_html=True)

# --- K·∫æT N·ªêI API & T·ª∞ ƒê·ªòNG QU√âT MODEL ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
    os.environ["GOOGLE_API_KEY"] = api_key
    
    # T·ª∞ ƒê·ªòNG QU√âT MODEL
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name:
                available_models.append(m.name)
    except: pass
    
    if not available_models:
        available_models = ["models/gemini-1.5-pro", "models/gemini-1.5-flash", "models/gemini-pro"]
        
except:
    st.error("‚ö†Ô∏è Ch∆∞a nh·∫≠p API Key trong Secrets.")
    st.stop()

# --- C√ÅC H√ÄM X·ª¨ L√ù ---

def get_text_from_files(files):
    text = ""
    for f in files:
        if f.name.endswith('.pdf'):
            reader = PdfReader(f)
            for page in reader.pages: text += page.extract_text() or ""
        elif f.name.endswith('.docx'):
            doc = Document(f)
            for para in doc.paragraphs: text += para.text + "\n"
        elif f.name.endswith('.txt'):
            text += f.getvalue().decode("utf-8")
    return text

def get_text_chunks(text):
    # S·ª≠ d·ª•ng h√†m ƒë√£ import ƒë√∫ng
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    chunks = text_splitter.split_text(text)
    return chunks

def create_vector_store(text_chunks):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index_huyenhoc")
    return vector_store

def zip_folder(folder_path, output_path):
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..')))

def scrape_chapter(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        content = "\n".join([p.get_text() for p in soup.find_all('p')])
        if len(content) < 100: content = soup.get_text()
        return content
    except: return ""

def translate_docx_preserve_layout(file, instruction, glossary, model_name):
    doc = Document(file)
    model_trans = genai.GenerativeModel(model_name)
    total_paragraphs = len(doc.paragraphs)
    bar = st.progress(0)
    status = st.empty()
    batch_size = 10
    current_batch = []
    current_indices = []
    
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            current_batch.append(text)
            current_indices.append(i)
        
        if len(current_batch) >= batch_size or (i == total_paragraphs - 1 and current_batch):
            status.text(f"ƒêang d·ªãch ƒëo·∫°n {i}/{total_paragraphs}...")
            batch_text = "\n[--BREAK--]\n".join(current_batch)
            prompt = f"VAI TR√í: Bi√™n d·ªãch vi√™n.\nNHI·ªÜM V·ª§: D·ªãch sang Ti·∫øng Vi·ªát.\nY√äU C·∫¶U: {instruction}\nTHU·∫¨T NG·ªÆ: {glossary}\nL∆ØU √ù: Gi·ªØ nguy√™n s·ªë l∆∞·ª£ng ƒëo·∫°n, ph√¢n c√°ch b·ªüi [--BREAK--].\n\nVƒÇN B·∫¢N G·ªêC:\n{batch_text}"
            try:
                response = model_trans.generate_content(prompt)
                translated_batch = response.text.split("[--BREAK--]")
                for idx, trans_text in zip(current_indices, translated_batch):
                    if idx < len(doc.paragraphs):
                        doc.paragraphs[idx].text = trans_text.strip()
            except: pass
            current_batch = []
            current_indices = []
            bar.progress((i+1)/total_paragraphs)
            time.sleep(1)

    status.text("‚úÖ ƒê√£ d·ªãch xong! ·∫¢nh v√† B·∫£ng bi·ªÉu ƒë∆∞·ª£c gi·ªØ nguy√™n.")
    bio = io.BytesIO()
    doc.save(bio)
    return bio

def save_docx_new(content):
    doc = Document()
    for line in content.split('\n'):
        if line.strip(): doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- GIAO DI·ªÜN CH√çNH ---
st.title("‚òØÔ∏è Ultimate AI: God Mode (Fixed)")

# --- SIDEBAR: C·∫§U H√åNH ---
with st.sidebar:
    st.header("‚öôÔ∏è TRUNG T√ÇM ƒêI·ªÄU KHI·ªÇN")
    
    selected_model_name = st.selectbox(
        "Ch·ªçn B·ªô N√£o (Model):",
        available_models,
        index=0
    )
    st.success(f"ƒêang d√πng: {selected_model_name}")
    st.divider()
    
    menu = st.radio("CH·ª®C NƒÇNG:", [
        "1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)",
        "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)",
        "3. D·ªãch Thu·∫≠t ƒêa NƒÉng (S√°ch/·∫¢nh/Link)"
    ])

# --- MODULE 1: HU·∫§N LUY·ªÜN ---
if menu == "1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)":
    st.header("üß† Hu·∫•n Luy·ªán AI")
    st.info("N·∫°p s√°ch Giang C√¥ng, Phong Th·ªßy (PDF/Docx) ƒë·ªÉ t·∫°o 'B·ªô N√£o'.")
    uploaded_files = st.file_uploader("N·∫°p s√°ch:", accept_multiple_files=True)
    if st.button("Train & T·∫£i B·ªô N√£o"):
        if uploaded_files:
            with st.spinner("ƒêang h·ªçc..."):
                raw = get_text_from_files(uploaded_files)
                create_vector_store(get_text_chunks(raw))
                zip_folder("faiss_index_huyenhoc", "bo_nao.zip")
                with open("bo_nao.zip", "rb") as fp:
                    st.download_button("üì• T·∫£i B·ªô N√£o V·ªÅ", fp, "bo_nao.zip", "application/zip")

# --- MODULE 2: H·ªéI ƒê√ÅP ---
elif menu == "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)":
    st.header(f"üîÆ H·ªèi ƒê√°p RAG (Model: {selected_model_name})")
    brain = st.sidebar.file_uploader("N·∫°p file 'bo_nao.zip':", type="zip")
    vs = None
    if brain:
        with open("temp.zip", "wb") as f: f.write(brain.getbuffer())
        with zipfile.ZipFile("temp.zip", "r") as z: z.extractall(".")
        vs = FAISS.load_local("faiss_index_huyenhoc", GoogleGenerativeAIEmbeddings(model="models/embedding-001"), allow_dangerous_deserialization=True)
        st.sidebar.success("ƒê√£ n·∫°p n√£o!")
    
    if "msgs" not in st.session_state: st.session_state.msgs = []
    for m in st.session_state.msgs: st.chat_message(m["role"]).markdown(m["content"])
    
    if q := st.chat_input("H·ªèi g√¨ ƒëi..."):
        st.session_state.msgs.append({"role": "user", "content": q})
        st.chat_message("user").markdown(q)
        if vs:
            docs = vs.similarity_search(q, k=4)
            chain = load_qa_chain(ChatGoogleGenerativeAI(model=selected_model_name), chain_type="stuff", prompt=PromptTemplate(template="D·ª±a v√†o s√°ch: {context}\nTr·∫£ l·ªùi: {question}", input_variables=["context", "question"]))
            res = chain({"input_documents": docs, "question": q}, return_only_outputs=True)
            st.session_state.msgs.append({"role": "assistant", "content": res["output_text"]})
            st.chat_message("assistant").markdown(res["output_text"])
        else: st.error("Ch∆∞a n·∫°p b·ªô n√£o!")

# --- MODULE 3: D·ªäCH THU·∫¨T ƒêA NƒÇNG ---
elif menu == "3. D·ªãch Thu·∫≠t ƒêa NƒÉng (S√°ch/·∫¢nh/Link)":
    st.header(f"üè≠ D·ªãch Thu·∫≠t (ƒê·ªông c∆°: {selected_model_name})")
    
    col_a, col_b = st.columns(2)
    with col_a:
        instruction = st.text_area("Y√™u c·∫ßu vƒÉn phong:", value="D·ªãch sang ti·∫øng Vi·ªát. S√°ch c·ªï ch·ªØ d·ªçc ƒë·ªçc t·ª´ ph·∫£i sang tr√°i. VƒÉn phong hay.", height=100)
    with col_b:
        glossary = st.text_area("T·ª´ ƒëi·ªÉn (Glossary):", value="Insight\nROI\nTr√∫c C∆°", height=100)

    tab1, tab2, tab3 = st.tabs(["üìÑ File Word (Gi·ªØ ·∫¢nh)", "üåê Link/Text", "üñºÔ∏è D·ªãch ·∫¢nh (OCR)"])

    with tab1:
        st.info("N·∫°p file Word (.docx). AI s·∫Ω d·ªãch ch·ªØ v√† GI·ªÆ NGUY√äN h√¨nh ·∫£nh/b·∫£ng bi·ªÉu.")
        docx_file = st.file_uploader("T·∫£i file Word:", type=['docx'])
        if docx_file and st.button("üöÄ D·ªãch File Word"):
            processed_file = translate_docx_preserve_layout(docx_file, instruction, glossary, selected_model_name)
            st.download_button(f"üì• T·∫£i v·ªÅ {docx_file.name}", processed_file.getvalue(), f"VN_{docx_file.name}", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

    with tab2:
        st.info("D√°n Link truy·ªán ho·∫∑c Text. AI s·∫Ω d·ªãch v√† t·∫°o file Word m·ªõi.")
        urls = st.text_area("D√°n Link (M·ªói d√≤ng 1 link):")
        if st.button("üöÄ D·ªãch Link"):
            links = urls.split('\n')
            full = ""
            bar = st.progress(0)
            model_t = genai.GenerativeModel(selected_model_name)
            for i, link in enumerate(links):
                if link.strip():
                    raw = scrape_chapter(link.strip())
                    if raw:
                        try:
                            prompt = f"Y√™u c·∫ßu: {instruction}\nThu·∫≠t ng·ªØ: {glossary}\nN·ªôi dung: {raw[:15000]}"
                            res = model_t.generate_content(prompt)
                            full += f"\n\n--- {link} ---\n{res.text}"
                        except: pass
                    bar.progress((i+1)/len(links))
            st.download_button("T·∫£i v·ªÅ (.docx)", save_docx_new(full).getvalue(), "Truyen_Web.docx")

    with tab3:
        st.info("T·∫£i ·∫£nh ch·ª•p s√°ch/truy·ªán. AI s·∫Ω nh·∫≠n di·ªán b·ªë c·ª•c d·ªçc/ngang v√† d·ªãch.")
        uploaded_imgs = st.file_uploader("T·∫£i ·∫£nh:", accept_multiple_files=True, type=['png', 'jpg', 'jpeg'])
        if uploaded_imgs and st.button("üöÄ D·ªãch ·∫¢nh"):
            full_trans = ""
            model_vision = genai.GenerativeModel(selected_model_name)
            for img_file in uploaded_imgs:
                img = Image.open(img_file)
                st.image(img, width=200, caption=img_file.name)
                
                prompt_vision = f"""
                B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ v√† Vision AI.
                1. Nh√¨n v√†o ·∫£nh, nh·∫≠n di·ªán b·ªë c·ª•c (d·ªçc/ngang) v√† ch·ªØ vi·∫øt.
                2. D·ªãch sang Ti·∫øng Vi·ªát.
                3. Y√äU C·∫¶U ƒê·∫∂C BI·ªÜT: {instruction}
                4. THU·∫¨T NG·ªÆ: {glossary}
                """
                try:
                    res = model_vision.generate_content([prompt_vision, img])
                    full_trans += f"\n\n--- ·∫¢nh {img_file.name} ---\n{res.text}"
                except Exception as e:
                    full_trans += f"\n[L·ªói ·∫£nh {img_file.name}: {e}]"
            
            st.text_area("K·∫øt qu·∫£:", full_trans, height=300)
            st.download_button("üì• T·∫£i b·∫£n d·ªãch ·∫¢nh (.docx)", save_docx_new(full_trans).getvalue(), "Dich_Anh.docx")
