import streamlit as st
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from PyPDF2 import PdfReader
from docx import Document
from PIL import Image
import io
import requests
from bs4 import BeautifulSoup
import time
import zipfile
import os

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Ultimate AI Final", page_icon="‚òØÔ∏è", layout="wide")
st.markdown("""<style>.stButton>button {background-color: #d35400; color: white;}</style>""", unsafe_allow_html=True)

# --- K·∫æT N·ªêI API ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
    os.environ["GOOGLE_API_KEY"] = api_key
except:
    st.error("‚ö†Ô∏è Ch∆∞a nh·∫≠p API Key trong Secrets.")
    st.stop()

# --- C√ÅC H√ÄM X·ª¨ L√ù (GI·ªÆ NGUY√äN) ---
def get_text_from_files(files):
    text = ""
    for f in files:
        if f.name.endswith('.pdf'):
            reader = PdfReader(f)
            for page in reader.pages: text += page.extract_text() or ""
        elif f.name.endswith('.docx'):
            doc = Document(f)
            for para in doc.paragraphs: text += para.text + "\n"
        elif f.name.endswith('.txt'):
            text += f.getvalue().decode("utf-8")
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    chunks = text_splitter.split_text(text)
    return chunks

def create_vector_store(text_chunks):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index_huyenhoc")
    return vector_store

def zip_folder(folder_path, output_path):
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..')))

def scrape_chapter(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        content = "\n".join([p.get_text() for p in soup.find_all('p')])
        if len(content) < 100: content = soup.get_text()
        return content
    except: return ""

def translate_docx_preserve_layout(file, instruction, glossary):
    doc = Document(file)
    model_trans = genai.GenerativeModel('gemini-1.5-flash')
    total_paragraphs = len(doc.paragraphs)
    bar = st.progress(0)
    status = st.empty()
    batch_size = 10
    current_batch = []
    current_indices = []
    
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            current_batch.append(text)
            current_indices.append(i)
        
        if len(current_batch) >= batch_size or (i == total_paragraphs - 1 and current_batch):
            status.text(f"ƒêang d·ªãch ƒëo·∫°n {i}/{total_paragraphs}...")
            batch_text = "\n[--BREAK--]\n".join(current_batch)
            prompt = f"VAI TR√í: Bi√™n d·ªãch vi√™n.\nNHI·ªÜM V·ª§: D·ªãch sang Ti·∫øng Vi·ªát.\nY√äU C·∫¶U: {instruction}\nTHU·∫¨T NG·ªÆ: {glossary}\nL∆ØU √ù: Gi·ªØ nguy√™n s·ªë l∆∞·ª£ng ƒëo·∫°n, ph√¢n c√°ch b·ªüi [--BREAK--].\n\nVƒÇN B·∫¢N G·ªêC:\n{batch_text}"
            try:
                response = model_trans.generate_content(prompt)
                translated_batch = response.text.split("[--BREAK--]")
                for idx, trans_text in zip(current_indices, translated_batch):
                    if idx < len(doc.paragraphs):
                        doc.paragraphs[idx].text = trans_text.strip()
            except: pass
            current_batch = []
            current_indices = []
            bar.progress((i+1)/total_paragraphs)
            time.sleep(1)

    status.text("‚úÖ ƒê√£ d·ªãch xong! ·∫¢nh v√† B·∫£ng bi·ªÉu ƒë∆∞·ª£c gi·ªØ nguy√™n.")
    bio = io.BytesIO()
    doc.save(bio)
    return bio

def save_docx_new(content):
    doc = Document()
    for line in content.split('\n'):
        if line.strip(): doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- GIAO DI·ªÜN CH√çNH ---
st.title("‚òØÔ∏è Ultimate AI: ƒê·∫°i S∆∞ & D·ªãch Gi·∫£")

menu = st.sidebar.radio("CH·ª®C NƒÇNG:", [
    "1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)",
    "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)",
    "3. D·ªãch Thu·∫≠t ƒêa NƒÉng (S√°ch/·∫¢nh/Link)"
])

# --- MODULE 1: HU·∫§N LUY·ªÜN ---
if menu == "1. Hu·∫•n Luy·ªán & L∆∞u Tr·ªØ (Train Brain)":
    st.header("üß† Hu·∫•n Luy·ªán AI")
    st.info("N·∫°p s√°ch Giang C√¥ng, Phong Th·ªßy (PDF/Docx) ƒë·ªÉ t·∫°o 'B·ªô N√£o'.")
    uploaded_files = st.file_uploader("N·∫°p s√°ch:", accept_multiple_files=True)
    if st.button("Train & T·∫£i B·ªô N√£o"):
        if uploaded_files:
            with st.spinner("ƒêang h·ªçc..."):
                raw = get_text_from_files(uploaded_files)
                create_vector_store(get_text_chunks(raw))
                zip_folder("faiss_index_huyenhoc", "bo_nao.zip")
                with open("bo_nao.zip", "rb") as fp:
                    st.download_button("üì• T·∫£i B·ªô N√£o V·ªÅ", fp, "bo_nao.zip", "application/zip")

# --- MODULE 2: H·ªéI ƒê√ÅP ---
elif menu == "2. H·ªèi ƒê·∫°i S∆∞ (D√πng B·ªô N√£o)":
    st.header("üîÆ H·ªèi ƒê√°p RAG")
    brain = st.sidebar.file_uploader("N·∫°p file 'bo_nao.zip':", type="zip")
    vs = None
    if brain:
        with open("temp.zip", "wb") as f: f.write(brain.getbuffer())
        with zipfile.ZipFile("temp.zip", "r") as z: z.extractall(".")
        vs = FAISS.load_local("faiss_index_huyenhoc", GoogleGenerativeAIEmbeddings(model="models/embedding-001"), allow_dangerous_deserialization=True)
        st.sidebar.success("ƒê√£ n·∫°p n√£o!")
    
    if "msgs" not in st.session_state: st.session_state.msgs = []
    for m in st.session_state.msgs: st.chat_message(m["role"]).markdown(m["content"])
    
    if q := st.chat_input("H·ªèi g√¨ ƒëi..."):
        st.session_state.msgs.append({"role": "user", "content": q})
        st.chat_message("user").markdown(q)
        if vs:
            docs = vs.similarity_search(q, k=4)
            chain = load_qa_chain(ChatGoogleGenerativeAI(model="gemini-1.5-pro"), chain_type="stuff", prompt=PromptTemplate(template="D·ª±a v√†o s√°ch: {context}\nTr·∫£ l·ªùi: {question}", input_variables=["context", "question"]))
            res = chain({"input_documents": docs, "question": q}, return_only_outputs=True)
            st.session_state.msgs.append({"role": "assistant", "content": res["output_text"]})
            st.chat_message("assistant").markdown(res["output_text"])
        else: st.error("Ch∆∞a n·∫°p b·ªô n√£o!")

# --- MODULE 3: D·ªäCH THU·∫¨T ƒêA NƒÇNG (ƒê·∫¶Y ƒê·ª¶ 3 TAB) ---
elif menu == "3. D·ªãch Thu·∫≠t ƒêa NƒÉng (S√°ch/·∫¢nh/Link)":
    st.header("üè≠ D·ªãch Thu·∫≠t C√¥ng Nghi·ªáp")
    
    col_a, col_b = st.columns(2)
    with col_a:
        instruction = st.text_area("Y√™u c·∫ßu vƒÉn phong:", value="D·ªãch sang ti·∫øng Vi·ªát. VƒÉn phong hay, d·ªÖ hi·ªÉu. Ki·ªÉm tra l·ªói ch√≠nh t·∫£ b·∫£n g·ªëc tr∆∞·ªõc khi d·ªãch.", height=100)
    with col_b:
        glossary = st.text_area("T·ª´ ƒëi·ªÉn (Glossary):", value="Insight\nROI\nTr√∫c C∆°", height=100)

    # ƒê√ÇY L√Ä PH·∫¶N QUAN TR·ªåNG B·∫†N C·∫¶N: 3 TAB
    tab1, tab2, tab3 = st.tabs(["üìÑ File Word (Gi·ªØ ·∫¢nh)", "üåê Link/Text", "üñºÔ∏è D·ªãch ·∫¢nh (OCR)"])

    # Tab 1: D·ªãch File Word gi·ªØ ƒë·ªãnh d·∫°ng
    with tab1:
        st.info("N·∫°p file Word (.docx). AI s·∫Ω d·ªãch ch·ªØ v√† GI·ªÆ NGUY√äN h√¨nh ·∫£nh/b·∫£ng bi·ªÉu.")
        docx_file = st.file_uploader("T·∫£i file Word:", type=['docx'])
        if docx_file and st.button("üöÄ D·ªãch File Word"):
            processed_file = translate_docx_preserve_layout(docx_file, instruction, glossary)
            st.download_button(f"üì• T·∫£i v·ªÅ {docx_file.name}", processed_file.getvalue(), f"VN_{docx_file.name}", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

    # Tab 2: D·ªãch Link ho·∫∑c Text
    with tab2:
        st.info("D√°n Link truy·ªán ho·∫∑c Text. AI s·∫Ω d·ªãch v√† t·∫°o file Word m·ªõi.")
        urls = st.text_area("D√°n Link (M·ªói d√≤ng 1 link):")
        if st.button("üöÄ D·ªãch Link"):
            links = urls.split('\n')
            full = ""
            bar = st.progress(0)
            model_t = genai.GenerativeModel('gemini-1.5-flash')
            for i, link in enumerate(links):
                if link.strip():
                    raw = scrape_chapter(link.strip())
                    if raw:
                        try:
                            prompt = f"Y√™u c·∫ßu: {instruction}\nThu·∫≠t ng·ªØ: {glossary}\nN·ªôi dung: {raw[:15000]}"
                            res = model_t.generate_content(prompt)
                            full += f"\n\n--- {link} ---\n{res.text}"
                        except: pass
                    bar.progress((i+1)/len(links))
            st.download_button("T·∫£i v·ªÅ (.docx)", save_docx_new(full).getvalue(), "Truyen_Web.docx")

    # Tab 3: D·ªãch ·∫¢nh (OCR) - ƒê√É TH√äM L·∫†I
    with tab3:
        st.info("T·∫£i ·∫£nh ch·ª•p s√°ch/truy·ªán (Ti·∫øng Trung/Anh). AI s·∫Ω nh·∫≠n di·ªán v√† d·ªãch.")
        uploaded_imgs = st.file_uploader("T·∫£i ·∫£nh:", accept_multiple_files=True, type=['png', 'jpg', 'jpeg'])
        if uploaded_imgs and st.button("üöÄ D·ªãch ·∫¢nh"):
            full_trans = ""
            model_vision = genai.GenerativeModel('gemini-1.5-flash')
            for img_file in uploaded_imgs:
                img = Image.open(img_file)
                st.image(img, width=200, caption=img_file.name)
                
                prompt_vision = f"""
                B·∫°n l√† chuy√™n gia ng√¥n ng·ªØ.
                1. Nh√¨n v√†o ·∫£nh, nh·∫≠n di·ªán to√†n b·ªô vƒÉn b·∫£n (k·ªÉ c·∫£ Ti·∫øng Trung ph·ªìn/gi·∫£n, Ti·∫øng Anh).
                2. D·ªãch sang Ti·∫øng Vi·ªát.
                3. Y√äU C·∫¶U: {instruction}
                4. THU·∫¨T NG·ªÆ: {glossary}
                """
                try:
                    res = model_vision.generate_content([prompt_vision, img])
                    full_trans += f"\n\n--- ·∫¢nh {img_file.name} ---\n{res.text}"
                except Exception as e:
                    full_trans += f"\n[L·ªói ·∫£nh {img_file.name}: {e}]"
            
            st.text_area("K·∫øt qu·∫£:", full_trans, height=300)
            st.download_button("üì• T·∫£i b·∫£n d·ªãch ·∫¢nh (.docx)", save_docx_new(full_trans).getvalue(), "Dich_Anh.docx")
